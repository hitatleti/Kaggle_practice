{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.　モデルチューニング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBMのハイパーパラメータのチューニング\n",
    "- scikit-learnのモデル利用\n",
    "- ニューラルネットワークの利用\n",
    "- アンサンブル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "# 分布の確認\n",
    "import pandas_profiling as pdp\n",
    "# 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "# 前処理\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "# モデリング\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# matplotlibで日本語表示したい場合はこれをinstallしてインポートする\n",
    "# !pip install japanize-matplotlib\n",
    "# import japanize_matplotlib\n",
    "# %matplotlib inline\n",
    "\n",
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "x_train, y_train, id_train = df_train[['Pclass', 'Fare']], df_train[[\n",
    "    'Survived']], df_train[['PassengerId']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1　LightGBMのハイパーパラメータのチューニング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1　手動チューニング\n",
    "1. 初期値の設定\n",
    "2. 学習結果に応じた個別チューニング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.2　自動チューニング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optunaを用いた自動チューニングの例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "     ------------------------------------- 365.3/365.3 kB 11.1 MB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "     ------------------------------------- 210.5/210.5 kB 12.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from optuna) (22.0)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.7/78.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詮索しないハイパーパラメータ\n",
    "params_base = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\":\"auc\",\n",
    "    \"learning_rate\": \"0.02\",\n",
    "    \"n_estimators\": 100000,\n",
    "    \"bagging_fleq\": 1,\n",
    "    \"seed\": 123,\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # 詮索するハイパーパラメータ\n",
    "    params_tuning = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 256),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 200),\n",
    "        \"min_sum_hessian_in_leaf\": trial.suggest_float(\"min_sum_hessian_in_leaf\", 1e-5, 1e-2, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-2, 1e2, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-2, 1e2, log=True),\n",
    "    }\n",
    "    params_tuning.update(params_base)\n",
    "\n",
    "    # モデル学習・評価\n",
    "    list_metrics = []\n",
    "    cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(x_train, y_train))\n",
    "    for nfold in np.arange(5):\n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        x_tr, y_tr = x_train.loc[idx_tr, :], y_train.loc[idx_tr, :]\n",
    "        x_va, y_va = x_train.loc[idx_va, :], y_train.loc[idx_va, :]\n",
    "        model = lgb.LGBMClassifier(**params_tuning)\n",
    "        model.fit(x_tr,\n",
    "                  y_tr, \n",
    "                  eval_set=[(x_tr, y_tr), (x_va, y_va)],\n",
    "                  early_stopping_rounds=100,\n",
    "                  verbose=0,)\n",
    "        y_va_pred = model.predict_proba(x_va)[:, 1]\n",
    "        metric_va = accuracy_score(y_va, np.where(y_va_pred>=0.5, 1, 0))\n",
    "        list_metrics.append(metric_va)\n",
    "\n",
    "    # 評価値の計算\n",
    "    metrics = np.mean(list_metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化処理（詮索の実行）\n",
    "sampler = optuna.samplers.TPESampler(seed=123)\n",
    "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc(best)=0.6992\n",
      "acc(best)=0.6992404745464817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 252,\n",
       " 'min_data_in_leaf': 60,\n",
       " 'min_sum_hessian_in_leaf': 0.009739830877756862,\n",
       " 'feature_fraction': 0.8018999555097835,\n",
       " 'bagging_fraction': 0.5949431124260618,\n",
       " 'lambda_l1': 0.1812977929299853,\n",
       " 'lambda_l2': 0.011197876549499167}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 詮索結果の確認\n",
    "trial = study.best_trial\n",
    "print(\"acc(best)={:,.4f}\".format(trial.value))\n",
    "print(f\"acc(best)={trial.value}\")\n",
    "display(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 252,\n",
       " 'min_data_in_leaf': 60,\n",
       " 'min_sum_hessian_in_leaf': 0.009739830877756862,\n",
       " 'feature_fraction': 0.8018999555097835,\n",
       " 'bagging_fraction': 0.5949431124260618,\n",
       " 'lambda_l1': 0.1812977929299853,\n",
       " 'lambda_l2': 0.011197876549499167,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'learning_rate': '0.02',\n",
       " 'n_estimators': 100000,\n",
       " 'bagging_fleq': 1,\n",
       " 'seed': 123}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ベストなハイパーパラメータの取得\n",
    "params_best = trial.params\n",
    "params_best.update(params_base)\n",
    "display(params_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2　LightGBM以外のモデル利用"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scikit-learnの各種モデル\n",
    "- ニューラルネットワーク"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1　scikit-learnの各種モデル"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的にはどのモデルでも同じ手順で処理できる.\n",
    "\n",
    "1. モデル定義: importした関数を指定してモデルを定義\n",
    "2. 学習: .fitで学習を実行する\n",
    "3. .predictで推論処理を実行する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearnの各種モデルでの学習の際の注意点\n",
    "- 欠損値を埋めないと学習できない\n",
    "- すべて数値データにしないと学習できない\n",
    "- 数値データを正規化あるいは標準化する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ロジスティック回帰"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡単のため説明変数は'Pclass', 'Age', 'Embarked'の３つとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの読み込み\n",
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "# データセットの作成\n",
    "x_train = df_train[['Pclass', 'Age', 'Embarked']]\n",
    "y_train = df_train[['Survived']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Age', 'Embarked'には欠損値があるので, 欠損値を保管する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値補間: 数値データ\n",
    "x_train['Age'] = x_train['Age'].fillna(x_train['Age'].mean())\n",
    "# 欠損値補間: カテゴリ変数\n",
    "x_train[\"Embarked\"] = x_train['Embarked'].fillna(x_train['Embarked'].mode()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリ変数である'Embarked'をohe-hot-encodingで数値データに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ変数の数値データへの変換（one-hot-encoding）\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(x_train[['Embarked']])\n",
    "df_embarked = pd.DataFrame(ohe.transform(x_train[['Embarked']]).toarray(), columns=[f\"Embarked_{col}\" for col in ohe.categories_[0]])\n",
    "x_train = pd.concat([x_train, df_embarked], axis=1)\n",
    "x_train = x_train.drop(columns=['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値データの正規化\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(df_train[['Pclass']])\n",
    "df_train['Pclass'] = mms.transform(df_train[['Pclass']])\n",
    "\n",
    "mms.fit(df_train[['Age']])\n",
    "df_train['Age'] = mms.transform(df_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 5) (179, 5) (712, 1) (179, 1)\n"
     ]
    }
   ],
   "source": [
    "# 学習データと検証データの分割（ホールドアウト検証）\n",
    "x_tr, x_va, y_tr, y_va = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=123)\n",
    "print(x_tr.shape, x_va.shape, y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7262569832402235\n",
      "[0 1 0 1 0]\n",
      "[[0.85951356 0.14048644]\n",
      " [0.20358813 0.79641187]\n",
      " [0.85501264 0.14498736]\n",
      " [0.28727379 0.71272621]\n",
      " [0.61610234 0.38389766]]\n"
     ]
    }
   ],
   "source": [
    "# モデル定義\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logis = LogisticRegression()\n",
    "\n",
    "# 学習\n",
    "model_logis.fit(x_tr, y_tr)\n",
    "\n",
    "# 予測\n",
    "y_va_pred = model_logis.predict(x_va)\n",
    "print(f\"accuracy: {accuracy_score(y_va, y_va_pred)}\")\n",
    "print(y_va_pred[:5])\n",
    "\n",
    "# 確率値の取得\n",
    "y_va_pred_prob = model_logis.predict_proba(x_va)\n",
    "print(y_va_pred_prob[:5, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM（サポートベクターマシン）\n",
    "今回はSVMの分類モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6368715083798883\n",
      "[0 0 0 0 0]\n",
      "[[0.66192458 0.33807542]\n",
      " [0.5599569  0.4400431 ]\n",
      " [0.66089667 0.33910333]\n",
      " [0.57490543 0.42509457]\n",
      " [0.58408982 0.41591018]]\n"
     ]
    }
   ],
   "source": [
    "# モデル定義\n",
    "from sklearn.svm import SVC\n",
    "model_svm = SVC(C=1.0, random_state=123, probability=True)\n",
    "\n",
    "# 学習\n",
    "model_svm.fit(x_tr, y_tr)\n",
    "\n",
    "# 予測\n",
    "y_va_pred = model_svm.predict(x_va)\n",
    "print(f\"accuracy: {accuracy_score(y_va, y_va_pred)}\")\n",
    "print(y_va_pred[:5])\n",
    "\n",
    "# 確率値の取得\n",
    "y_va_pred_prob = model_svm.predict_proba(x_va)\n",
    "print(y_va_pred_prob[:5, :])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2　ニューラルネットワーク"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークを用いて学習する際の注意点\n",
    "- 欠損値を埋めないと学習できない\n",
    "- すべて数値データにしないと学習できない\n",
    "- 数値データを正規化あるいは標準化する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全結合層のみのニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 9.2 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ---------------------------------------- 126.5/126.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     ------------------------------------- 895.9/895.9 kB 27.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     ------------------------------------- 439.2/439.2 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 18.2 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     --------------------------------------- 23.2/23.2 MB 16.8 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 21.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.1-py2.py3-none-any.whl (177 kB)\n",
      "     ------------------------------------ 177.2/177.2 kB 486.9 kB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ------------------------------------- 781.3/781.3 kB 16.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hisan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 keras-2.11.0 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\hisan\\anaconda3\\lib\\site-packages (2.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflowライブラリのimport\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflowの再現性のためのシード設定\n",
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_coef = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_coef)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの読み込み\n",
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# データセット作成\n",
    "x_train = df_train[[\"Pclass\", \"Age\", \"Embarked\"]]      # 簡単のため, 説明変数は3つ\n",
    "y_train = df_train[[\"Survived\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"数値データの前処理\"\"\"\n",
    "# 欠損値補間\n",
    "x_train[\"Age\"] = x_train[\"Age\"].fillna(x_train[\"Age\"].mean())\n",
    "# 正規化\n",
    "for col in [\"Pclass\", \"Age\"]:\n",
    "    value_min = x_train[col].min()\n",
    "    value_max = x_train[col].max()\n",
    "    x_train[col] = (x_train[col] - value_min) / (value_max - value_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"カテゴリ変数の前処理\"\"\"\n",
    "# 欠損値補間\n",
    "x_train[\"Embarked\"] = x_train[\"Embarked\"].fillna(x_train[\"Embarked\"].mode()[0])\n",
    "# one-hot-encoding\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(x_train[[\"Embarked\"]])\n",
    "df_embarked = pd.DataFrame(ohe.transform(x_train[[\"Embarked\"]]).toarray(), columns=[f\"Embarked_{col}\" for col in ohe.categories_[0]])\n",
    "x_train = pd.concat([x_train.drop(columns=[\"Embarked\"]), df_embarked], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 5) (179, 5) (712, 1) (179, 1)\n"
     ]
    }
   ],
   "source": [
    "# 学習データとテストデータの分割\n",
    "x_tr, x_va, y_tr, y_va = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=123)\n",
    "print(x_tr.shape, x_va.shape, y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                60        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10)               40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5)                20        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 281\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    input_num = Input(shape=(5,))\n",
    "    x_num = Dense(10, activation=\"relu\")(input_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.3)(x_num)\n",
    "    x_num = Dense(10, activation=\"relu\")(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.2)(x_num)\n",
    "    x_num = Dense(5, activation=\"relu\")(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.1)(x_num)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x_num)\n",
    "    \n",
    "    model = Model(inputs=input_num, outputs=out,)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"binary_crossentropy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "50/89 [===============>..............] - ETA: 0s - loss: 0.7558 - binary_crossentropy: 0.7558  \n",
      "Epoch 1: val_loss improved from inf to 0.68177, saving model to model_keras.h5\n",
      "89/89 [==============================] - 1s 3ms/step - loss: 0.7269 - binary_crossentropy: 0.7269 - val_loss: 0.6818 - val_binary_crossentropy: 0.6818 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "60/89 [===================>..........] - ETA: 0s - loss: 0.6786 - binary_crossentropy: 0.6786\n",
      "Epoch 2: val_loss improved from 0.68177 to 0.66743, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6733 - binary_crossentropy: 0.6733 - val_loss: 0.6674 - val_binary_crossentropy: 0.6674 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "61/89 [===================>..........] - ETA: 0s - loss: 0.6895 - binary_crossentropy: 0.6895\n",
      "Epoch 3: val_loss improved from 0.66743 to 0.65251, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6751 - binary_crossentropy: 0.6751 - val_loss: 0.6525 - val_binary_crossentropy: 0.6525 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "65/89 [====================>.........] - ETA: 0s - loss: 0.6391 - binary_crossentropy: 0.6391\n",
      "Epoch 4: val_loss improved from 0.65251 to 0.63584, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6488 - binary_crossentropy: 0.6488 - val_loss: 0.6358 - val_binary_crossentropy: 0.6358 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "50/89 [===============>..............] - ETA: 0s - loss: 0.6628 - binary_crossentropy: 0.6628\n",
      "Epoch 5: val_loss improved from 0.63584 to 0.61953, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6567 - binary_crossentropy: 0.6567 - val_loss: 0.6195 - val_binary_crossentropy: 0.6195 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "62/89 [===================>..........] - ETA: 0s - loss: 0.6588 - binary_crossentropy: 0.6588\n",
      "Epoch 6: val_loss improved from 0.61953 to 0.61322, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6626 - binary_crossentropy: 0.6626 - val_loss: 0.6132 - val_binary_crossentropy: 0.6132 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6526 - binary_crossentropy: 0.6526\n",
      "Epoch 7: val_loss did not improve from 0.61322\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6604 - binary_crossentropy: 0.6604 - val_loss: 0.6139 - val_binary_crossentropy: 0.6139 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6475 - binary_crossentropy: 0.6475\n",
      "Epoch 8: val_loss improved from 0.61322 to 0.61172, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6404 - binary_crossentropy: 0.6404 - val_loss: 0.6117 - val_binary_crossentropy: 0.6117 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6474 - binary_crossentropy: 0.6474\n",
      "Epoch 9: val_loss improved from 0.61172 to 0.60519, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6438 - binary_crossentropy: 0.6438 - val_loss: 0.6052 - val_binary_crossentropy: 0.6052 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "64/89 [====================>.........] - ETA: 0s - loss: 0.6390 - binary_crossentropy: 0.6390\n",
      "Epoch 10: val_loss did not improve from 0.60519\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6269 - binary_crossentropy: 0.6269 - val_loss: 0.6056 - val_binary_crossentropy: 0.6056 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6393 - binary_crossentropy: 0.6393\n",
      "Epoch 11: val_loss improved from 0.60519 to 0.60269, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6453 - binary_crossentropy: 0.6453 - val_loss: 0.6027 - val_binary_crossentropy: 0.6027 - lr: 0.0010\n",
      "Epoch 12/10000\n",
      "67/89 [=====================>........] - ETA: 0s - loss: 0.6346 - binary_crossentropy: 0.6346\n",
      "Epoch 12: val_loss improved from 0.60269 to 0.60190, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6310 - binary_crossentropy: 0.6310 - val_loss: 0.6019 - val_binary_crossentropy: 0.6019 - lr: 0.0010\n",
      "Epoch 13/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6446 - binary_crossentropy: 0.6446\n",
      "Epoch 13: val_loss did not improve from 0.60190\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6374 - binary_crossentropy: 0.6374 - val_loss: 0.6033 - val_binary_crossentropy: 0.6033 - lr: 0.0010\n",
      "Epoch 14/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.6251 - binary_crossentropy: 0.6251\n",
      "Epoch 14: val_loss did not improve from 0.60190\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6224 - binary_crossentropy: 0.6224 - val_loss: 0.6028 - val_binary_crossentropy: 0.6028 - lr: 0.0010\n",
      "Epoch 15/10000\n",
      "67/89 [=====================>........] - ETA: 0s - loss: 0.6207 - binary_crossentropy: 0.6207\n",
      "Epoch 15: val_loss did not improve from 0.60190\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6224 - binary_crossentropy: 0.6224 - val_loss: 0.6033 - val_binary_crossentropy: 0.6033 - lr: 0.0010\n",
      "Epoch 16/10000\n",
      "57/89 [==================>...........] - ETA: 0s - loss: 0.6023 - binary_crossentropy: 0.6023\n",
      "Epoch 16: val_loss improved from 0.60190 to 0.59457, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6083 - binary_crossentropy: 0.6083 - val_loss: 0.5946 - val_binary_crossentropy: 0.5946 - lr: 0.0010\n",
      "Epoch 17/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.6098 - binary_crossentropy: 0.6098\n",
      "Epoch 17: val_loss improved from 0.59457 to 0.59192, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6212 - binary_crossentropy: 0.6212 - val_loss: 0.5919 - val_binary_crossentropy: 0.5919 - lr: 0.0010\n",
      "Epoch 18/10000\n",
      "66/89 [=====================>........] - ETA: 0s - loss: 0.6262 - binary_crossentropy: 0.6262\n",
      "Epoch 18: val_loss did not improve from 0.59192\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6200 - binary_crossentropy: 0.6200 - val_loss: 0.5923 - val_binary_crossentropy: 0.5923 - lr: 0.0010\n",
      "Epoch 19/10000\n",
      "72/89 [=======================>......] - ETA: 0s - loss: 0.6149 - binary_crossentropy: 0.6149\n",
      "Epoch 19: val_loss did not improve from 0.59192\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6121 - binary_crossentropy: 0.6121 - val_loss: 0.5947 - val_binary_crossentropy: 0.5947 - lr: 0.0010\n",
      "Epoch 20/10000\n",
      "73/89 [=======================>......] - ETA: 0s - loss: 0.6135 - binary_crossentropy: 0.6135\n",
      "Epoch 20: val_loss did not improve from 0.59192\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6192 - binary_crossentropy: 0.6192 - val_loss: 0.5953 - val_binary_crossentropy: 0.5953 - lr: 0.0010\n",
      "Epoch 21/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.6231 - binary_crossentropy: 0.6231\n",
      "Epoch 21: val_loss did not improve from 0.59192\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6263 - binary_crossentropy: 0.6263 - val_loss: 0.5956 - val_binary_crossentropy: 0.5956 - lr: 0.0010\n",
      "Epoch 22/10000\n",
      "66/89 [=====================>........] - ETA: 0s - loss: 0.6276 - binary_crossentropy: 0.6276\n",
      "Epoch 22: val_loss did not improve from 0.59192\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6230 - binary_crossentropy: 0.6230 - val_loss: 0.5926 - val_binary_crossentropy: 0.5926 - lr: 0.0010\n",
      "Epoch 23/10000\n",
      "68/89 [=====================>........] - ETA: 0s - loss: 0.6088 - binary_crossentropy: 0.6088\n",
      "Epoch 23: val_loss did not improve from 0.59192\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6149 - binary_crossentropy: 0.6149 - val_loss: 0.5931 - val_binary_crossentropy: 0.5931 - lr: 1.0000e-04\n",
      "Epoch 24/10000\n",
      "67/89 [=====================>........] - ETA: 0s - loss: 0.6147 - binary_crossentropy: 0.6147\n",
      "Epoch 24: val_loss did not improve from 0.59192\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6232 - binary_crossentropy: 0.6232 - val_loss: 0.5921 - val_binary_crossentropy: 0.5921 - lr: 1.0000e-04\n",
      "Epoch 25/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.6199 - binary_crossentropy: 0.6199\n",
      "Epoch 25: val_loss improved from 0.59192 to 0.59014, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6229 - binary_crossentropy: 0.6229 - val_loss: 0.5901 - val_binary_crossentropy: 0.5901 - lr: 1.0000e-04\n",
      "Epoch 26/10000\n",
      "68/89 [=====================>........] - ETA: 0s - loss: 0.6411 - binary_crossentropy: 0.6411\n",
      "Epoch 26: val_loss did not improve from 0.59014\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6233 - binary_crossentropy: 0.6233 - val_loss: 0.5904 - val_binary_crossentropy: 0.5904 - lr: 1.0000e-04\n",
      "Epoch 27/10000\n",
      "70/89 [======================>.......] - ETA: 0s - loss: 0.6095 - binary_crossentropy: 0.6095\n",
      "Epoch 27: val_loss did not improve from 0.59014\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6197 - binary_crossentropy: 0.6197 - val_loss: 0.5904 - val_binary_crossentropy: 0.5904 - lr: 1.0000e-04\n",
      "Epoch 28/10000\n",
      "58/89 [==================>...........] - ETA: 0s - loss: 0.6258 - binary_crossentropy: 0.6258\n",
      "Epoch 28: val_loss did not improve from 0.59014\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6241 - binary_crossentropy: 0.6241 - val_loss: 0.5915 - val_binary_crossentropy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 29/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6091 - binary_crossentropy: 0.6091\n",
      "Epoch 29: val_loss did not improve from 0.59014\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6097 - binary_crossentropy: 0.6097 - val_loss: 0.5910 - val_binary_crossentropy: 0.5910 - lr: 1.0000e-04\n",
      "Epoch 30/10000\n",
      "67/89 [=====================>........] - ETA: 0s - loss: 0.5937 - binary_crossentropy: 0.5937\n",
      "Epoch 30: val_loss did not improve from 0.59014\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5993 - binary_crossentropy: 0.5993 - val_loss: 0.5911 - val_binary_crossentropy: 0.5911 - lr: 1.0000e-04\n",
      "Epoch 31/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6419 - binary_crossentropy: 0.6419\n",
      "Epoch 31: val_loss did not improve from 0.59014\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6363 - binary_crossentropy: 0.6363 - val_loss: 0.5908 - val_binary_crossentropy: 0.5908 - lr: 1.0000e-05\n",
      "Epoch 32/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.6211 - binary_crossentropy: 0.6211\n",
      "Epoch 32: val_loss improved from 0.59014 to 0.58975, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6273 - binary_crossentropy: 0.6273 - val_loss: 0.5898 - val_binary_crossentropy: 0.5898 - lr: 1.0000e-05\n",
      "Epoch 33/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6270 - binary_crossentropy: 0.6270\n",
      "Epoch 33: val_loss improved from 0.58975 to 0.58971, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6260 - binary_crossentropy: 0.6260 - val_loss: 0.5897 - val_binary_crossentropy: 0.5897 - lr: 1.0000e-05\n",
      "Epoch 34/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6272 - binary_crossentropy: 0.6272\n",
      "Epoch 34: val_loss did not improve from 0.58971\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6233 - binary_crossentropy: 0.6233 - val_loss: 0.5900 - val_binary_crossentropy: 0.5900 - lr: 1.0000e-05\n",
      "Epoch 35/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.6321 - binary_crossentropy: 0.6321\n",
      "Epoch 35: val_loss did not improve from 0.58971\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6292 - binary_crossentropy: 0.6292 - val_loss: 0.5902 - val_binary_crossentropy: 0.5902 - lr: 1.0000e-05\n",
      "Epoch 36/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6184 - binary_crossentropy: 0.6184\n",
      "Epoch 36: val_loss did not improve from 0.58971\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6245 - binary_crossentropy: 0.6245 - val_loss: 0.5906 - val_binary_crossentropy: 0.5906 - lr: 1.0000e-05\n",
      "Epoch 37/10000\n",
      "67/89 [=====================>........] - ETA: 0s - loss: 0.5984 - binary_crossentropy: 0.5984\n",
      "Epoch 37: val_loss improved from 0.58971 to 0.58925, saving model to model_keras.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6093 - binary_crossentropy: 0.6093 - val_loss: 0.5893 - val_binary_crossentropy: 0.5893 - lr: 1.0000e-05\n",
      "Epoch 38/10000\n",
      "54/89 [=================>............] - ETA: 0s - loss: 0.6009 - binary_crossentropy: 0.6009\n",
      "Epoch 38: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6083 - binary_crossentropy: 0.6083 - val_loss: 0.5898 - val_binary_crossentropy: 0.5898 - lr: 1.0000e-05\n",
      "Epoch 39/10000\n",
      "68/89 [=====================>........] - ETA: 0s - loss: 0.6301 - binary_crossentropy: 0.6301\n",
      "Epoch 39: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6245 - binary_crossentropy: 0.6245 - val_loss: 0.5903 - val_binary_crossentropy: 0.5903 - lr: 1.0000e-05\n",
      "Epoch 40/10000\n",
      "56/89 [=================>............] - ETA: 0s - loss: 0.6222 - binary_crossentropy: 0.6222\n",
      "Epoch 40: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6165 - binary_crossentropy: 0.6165 - val_loss: 0.5904 - val_binary_crossentropy: 0.5904 - lr: 1.0000e-05\n",
      "Epoch 41/10000\n",
      "68/89 [=====================>........] - ETA: 0s - loss: 0.6034 - binary_crossentropy: 0.6034\n",
      "Epoch 41: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6142 - binary_crossentropy: 0.6142 - val_loss: 0.5898 - val_binary_crossentropy: 0.5898 - lr: 1.0000e-05\n",
      "Epoch 42/10000\n",
      "70/89 [======================>.......] - ETA: 0s - loss: 0.5995 - binary_crossentropy: 0.5995\n",
      "Epoch 42: val_loss did not improve from 0.58925\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6134 - binary_crossentropy: 0.6134 - val_loss: 0.5903 - val_binary_crossentropy: 0.5903 - lr: 1.0000e-05\n",
      "Epoch 43/10000\n",
      "68/89 [=====================>........] - ETA: 0s - loss: 0.6216 - binary_crossentropy: 0.6216\n",
      "Epoch 43: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6250 - binary_crossentropy: 0.6250 - val_loss: 0.5904 - val_binary_crossentropy: 0.5904 - lr: 1.0000e-06\n",
      "Epoch 44/10000\n",
      "72/89 [=======================>......] - ETA: 0s - loss: 0.6346 - binary_crossentropy: 0.6346\n",
      "Epoch 44: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6192 - binary_crossentropy: 0.6192 - val_loss: 0.5893 - val_binary_crossentropy: 0.5893 - lr: 1.0000e-06\n",
      "Epoch 45/10000\n",
      "70/89 [======================>.......] - ETA: 0s - loss: 0.6427 - binary_crossentropy: 0.6427\n",
      "Epoch 45: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6324 - binary_crossentropy: 0.6324 - val_loss: 0.5894 - val_binary_crossentropy: 0.5894 - lr: 1.0000e-06\n",
      "Epoch 46/10000\n",
      "69/89 [======================>.......] - ETA: 0s - loss: 0.6146 - binary_crossentropy: 0.6146\n",
      "Epoch 46: val_loss did not improve from 0.58925\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6121 - binary_crossentropy: 0.6121 - val_loss: 0.5897 - val_binary_crossentropy: 0.5897 - lr: 1.0000e-06\n",
      "Epoch 47/10000\n",
      "71/89 [======================>.......] - ETA: 0s - loss: 0.5933 - binary_crossentropy: 0.5933\n",
      "Epoch 47: val_loss did not improve from 0.58925\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6001 - binary_crossentropy: 0.6001 - val_loss: 0.5902 - val_binary_crossentropy: 0.5902 - lr: 1.0000e-06\n",
      "Epoch 47: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c611791400>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデル学習\n",
    "seed_everything(seed=123)\n",
    "model = create_model()\n",
    "model.fit(x=x_tr,\n",
    "          y=y_tr,\n",
    "          validation_data=(x_va, y_va),\n",
    "          batch_size=8,\n",
    "          epochs=10000,\n",
    "          callbacks=[ModelCheckpoint(filepath=\"model_keras.h5\", monitor=\"val_loss\", mode=\"min\", verbose=1, save_best_only=True, save_weights_only=True),\n",
    "                     EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=0, patience=10, verbose=1, restore_best_weights=True), \n",
    "                     ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.1, patience=5, verbose=1),],\n",
    "          verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 636us/step\n",
      "accuracy: 0.692737\n"
     ]
    }
   ],
   "source": [
    "# モデルの評価\n",
    "y_va_pred = model.predict(x_va, batch_size=8, verbose=1)\n",
    "print(f\"accuracy: {accuracy_score(y_va, np.where(y_va_pred>=0.5, 1, 0)):f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込み層ありのネットワークモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "x_train = df_train[[\"Pclass\", \"Age\", \"Cabin\"]]\n",
    "y_train = df_train[[\"Survived\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"数値データの前処理\"\"\"\n",
    "# 欠損値補間\n",
    "x_train[\"Age\"] = x_train[\"Age\"].fillna(x_train[\"Age\"].mean())\n",
    "# 正規化\n",
    "for col in [\"Pclass\", \"Age\"]:\n",
    "    value_min = x_train[col].min()\n",
    "    value_max = x_train[col].max()\n",
    "    x_train[col] = (x_train[col] - value_min) / (value_max - value_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A10' 'A14' 'A16' 'A19' 'A20' 'A23' 'A24' 'A26' 'A31' 'A32' 'A34' 'A36'\n",
      " 'A5' 'A6' 'A7' 'B101' 'B102' 'B18' 'B19' 'B20' 'B22' 'B28' 'B3' 'B30'\n",
      " 'B35' 'B37' 'B38' 'B39' 'B4' 'B41' 'B42' 'B49' 'B5' 'B50' 'B51 B53 B55'\n",
      " 'B57 B59 B63 B66' 'B58 B60' 'B69' 'B71' 'B73' 'B77' 'B78' 'B79' 'B80'\n",
      " 'B82 B84' 'B86' 'B94' 'B96 B98' 'C101' 'C103' 'C104' 'C106' 'C110' 'C111'\n",
      " 'C118' 'C123' 'C124' 'C125' 'C126' 'C128' 'C148' 'C2' 'C22 C26'\n",
      " 'C23 C25 C27' 'C30' 'C32' 'C45' 'C46' 'C47' 'C49' 'C50' 'C52' 'C54'\n",
      " 'C62 C64' 'C65' 'C68' 'C7' 'C70' 'C78' 'C82' 'C83' 'C85' 'C86' 'C87'\n",
      " 'C90' 'C91' 'C92' 'C93' 'C95' 'C99' 'D' 'D10 D12' 'D11' 'D15' 'D17' 'D19'\n",
      " 'D20' 'D21' 'D26' 'D28' 'D30' 'D33' 'D35' 'D36' 'D37' 'D45' 'D46' 'D47'\n",
      " 'D48' 'D49' 'D50' 'D56' 'D6' 'D7' 'D9' 'E10' 'E101' 'E12' 'E121' 'E17'\n",
      " 'E24' 'E25' 'E31' 'E33' 'E34' 'E36' 'E38' 'E40' 'E44' 'E46' 'E49' 'E50'\n",
      " 'E58' 'E63' 'E67' 'E68' 'E77' 'E8' 'F E69' 'F G63' 'F G73' 'F2' 'F33'\n",
      " 'F38' 'F4' 'G6' 'None' 'T']\n",
      "count: 148\n"
     ]
    }
   ],
   "source": [
    "\"\"\"カテゴリ変数の前処理\"\"\"\n",
    "# 欠損値補間\n",
    "x_train[\"Cabin\"] = x_train[\"Cabin\"].fillna(\"None\")\n",
    "# label-encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(x_train[[\"Cabin\"]])\n",
    "x_train[\"Cabin\"] = le.transform(x_train[\"Cabin\"])\n",
    "\n",
    "print(le.classes_)\n",
    "print(\"count:\", len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 2) (179, 2) (712, 1) (179, 1) (712, 1) (179, 1)\n"
     ]
    }
   ],
   "source": [
    "# 学習データと検証データの分割\n",
    "x_train_num, x_train_cat = x_train[[\"Pclass\", \"Age\"]], x_train[[\"Cabin\"]]\n",
    "\n",
    "x_num_tr, x_num_va, x_cat_tr, x_cat_va, y_tr, y_va = train_test_split(x_train_num, x_train_cat, y_train, test_size=0.2, stratify=y_train, random_state=123)\n",
    "print(x_num_tr.shape, x_num_va.shape, x_cat_tr.shape, x_cat_va.shape, y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 10)           30          ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None,)             0           ['input_6[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 10)          40          ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 74)           10952       ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 10)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 74)           0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 10)           110         ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 74)           0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 84)           0           ['dense_17[0][0]',               \n",
      "                                                                  'flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 50)           4250        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 50)          200         ['dense_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 50)           0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 20)           1020        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 20)          80          ['dense_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 20)           0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 1)            21          ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,703\n",
      "Trainable params: 16,543\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "def create_model_embedding():\n",
    "    ########### num\n",
    "    input_num = Input(shape=(2,))\n",
    "    layer_num = Dense(10, activation=\"relu\")(input_num)\n",
    "    layer_num = BatchNormalization()(layer_num)\n",
    "    layer_num = Dropout(0.2)(layer_num)\n",
    "    layer_num = Dense(10, activation=\"relu\")(layer_num)\n",
    "    \n",
    "    ########### cat\n",
    "    input_cat = Input(shape=(1,))\n",
    "    layer_cat = input_cat[:,0]\n",
    "    layer_cat = Embedding(input_dim=148, output_dim=74)(layer_cat)\n",
    "    layer_cat = Dropout(0.2)(layer_cat)\n",
    "    layer_cat = Flatten()(layer_cat)\n",
    "    \n",
    "    ########### concat\n",
    "    hidden_layer = Concatenate()([layer_num, layer_cat])\n",
    "    hidden_layer = Dense(50, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = BatchNormalization()(hidden_layer)\n",
    "    hidden_layer = Dropout(0.1)(hidden_layer)\n",
    "    hidden_layer = Dense(20, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = BatchNormalization()(hidden_layer)\n",
    "    hidden_layer = Dropout(0.1)(hidden_layer)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    \n",
    "    model = Model(inputs=[input_num, input_cat], outputs=output_layer,)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"binary_crossentropy\"],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model_embedding()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "51/89 [================>.............] - ETA: 0s - loss: 0.8254 - binary_crossentropy: 0.8254  \n",
      "Epoch 1: val_loss improved from inf to 0.65927, saving model to model_keras_embedding.h5\n",
      "89/89 [==============================] - 1s 3ms/step - loss: 0.7856 - binary_crossentropy: 0.7856 - val_loss: 0.6593 - val_binary_crossentropy: 0.6593 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "59/89 [==================>...........] - ETA: 0s - loss: 0.6552 - binary_crossentropy: 0.6552\n",
      "Epoch 2: val_loss improved from 0.65927 to 0.65246, saving model to model_keras_embedding.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6652 - binary_crossentropy: 0.6652 - val_loss: 0.6525 - val_binary_crossentropy: 0.6525 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "57/89 [==================>...........] - ETA: 0s - loss: 0.6790 - binary_crossentropy: 0.6790\n",
      "Epoch 3: val_loss improved from 0.65246 to 0.65015, saving model to model_keras_embedding.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6523 - binary_crossentropy: 0.6523 - val_loss: 0.6502 - val_binary_crossentropy: 0.6502 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "56/89 [=================>............] - ETA: 0s - loss: 0.6377 - binary_crossentropy: 0.6377\n",
      "Epoch 4: val_loss improved from 0.65015 to 0.63424, saving model to model_keras_embedding.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.6340 - binary_crossentropy: 0.6340 - val_loss: 0.6342 - val_binary_crossentropy: 0.6342 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "58/89 [==================>...........] - ETA: 0s - loss: 0.6126 - binary_crossentropy: 0.6126\n",
      "Epoch 5: val_loss improved from 0.63424 to 0.61907, saving model to model_keras_embedding.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5943 - binary_crossentropy: 0.5943 - val_loss: 0.6191 - val_binary_crossentropy: 0.6191 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "57/89 [==================>...........] - ETA: 0s - loss: 0.5870 - binary_crossentropy: 0.5870\n",
      "Epoch 6: val_loss improved from 0.61907 to 0.59948, saving model to model_keras_embedding.h5\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5878 - binary_crossentropy: 0.5878 - val_loss: 0.5995 - val_binary_crossentropy: 0.5995 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "60/89 [===================>..........] - ETA: 0s - loss: 0.5602 - binary_crossentropy: 0.5602\n",
      "Epoch 7: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5841 - binary_crossentropy: 0.5841 - val_loss: 0.6071 - val_binary_crossentropy: 0.6071 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "60/89 [===================>..........] - ETA: 0s - loss: 0.5781 - binary_crossentropy: 0.5781\n",
      "Epoch 8: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5684 - binary_crossentropy: 0.5684 - val_loss: 0.6384 - val_binary_crossentropy: 0.6384 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "60/89 [===================>..........] - ETA: 0s - loss: 0.5455 - binary_crossentropy: 0.5455\n",
      "Epoch 9: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5445 - binary_crossentropy: 0.5445 - val_loss: 0.6532 - val_binary_crossentropy: 0.6532 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "61/89 [===================>..........] - ETA: 0s - loss: 0.5612 - binary_crossentropy: 0.5612\n",
      "Epoch 10: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5548 - binary_crossentropy: 0.5548 - val_loss: 0.6396 - val_binary_crossentropy: 0.6396 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "61/89 [===================>..........] - ETA: 0s - loss: 0.5184 - binary_crossentropy: 0.5184\n",
      "Epoch 11: val_loss did not improve from 0.59948\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5435 - binary_crossentropy: 0.5435 - val_loss: 0.6498 - val_binary_crossentropy: 0.6498 - lr: 0.0010\n",
      "Epoch 12/10000\n",
      "56/89 [=================>............] - ETA: 0s - loss: 0.5486 - binary_crossentropy: 0.5486\n",
      "Epoch 12: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5305 - binary_crossentropy: 0.5305 - val_loss: 0.6503 - val_binary_crossentropy: 0.6503 - lr: 1.0000e-04\n",
      "Epoch 13/10000\n",
      "58/89 [==================>...........] - ETA: 0s - loss: 0.5333 - binary_crossentropy: 0.5333\n",
      "Epoch 13: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5552 - binary_crossentropy: 0.5552 - val_loss: 0.6532 - val_binary_crossentropy: 0.6532 - lr: 1.0000e-04\n",
      "Epoch 14/10000\n",
      "60/89 [===================>..........] - ETA: 0s - loss: 0.5415 - binary_crossentropy: 0.5415\n",
      "Epoch 14: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5463 - binary_crossentropy: 0.5463 - val_loss: 0.6569 - val_binary_crossentropy: 0.6569 - lr: 1.0000e-04\n",
      "Epoch 15/10000\n",
      "60/89 [===================>..........] - ETA: 0s - loss: 0.5235 - binary_crossentropy: 0.5235\n",
      "Epoch 15: val_loss did not improve from 0.59948\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5399 - binary_crossentropy: 0.5399 - val_loss: 0.6549 - val_binary_crossentropy: 0.6549 - lr: 1.0000e-04\n",
      "Epoch 16/10000\n",
      "61/89 [===================>..........] - ETA: 0s - loss: 0.5470 - binary_crossentropy: 0.5470\n",
      "Epoch 16: val_loss did not improve from 0.59948\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "89/89 [==============================] - 0s 1ms/step - loss: 0.5575 - binary_crossentropy: 0.5575 - val_loss: 0.6558 - val_binary_crossentropy: 0.6558 - lr: 1.0000e-04\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c615d20610>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの学習\n",
    "seed_everything(seed=123)\n",
    "model = create_model_embedding()\n",
    "model.fit(x=[x_num_tr, x_cat_tr],\n",
    "          y=y_tr,\n",
    "          validation_data=([x_num_va, x_cat_va], y_va),\n",
    "          batch_size=8,\n",
    "          epochs=10000,\n",
    "          callbacks=[ModelCheckpoint(filepath=\"model_keras_embedding.h5\", monitor=\"val_loss\", mode=\"min\", verbose=1, save_best_only=True, save_weights_only=True),\n",
    "                     EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=0, patience=10, verbose=1, restore_best_weights=True), \n",
    "                     ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.1, patience=5, verbose=1),],\n",
    "          verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 591us/step\n",
      "accuracy:0.703911\n"
     ]
    }
   ],
   "source": [
    "# モデルの評価\n",
    "y_va_pred = model.predict([x_num_va, x_cat_va], batch_size=8, verbose=1)\n",
    "print(f\"accuracy:{accuracy_score(y_va, np.where(y_va_pred>=0.5,1,0)):f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3　アンサンブル"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 単純平均\n",
    "- 重み付き平均\n",
    "- スタッキング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.1　単純平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.668853</td>\n",
       "      <td>0.953560</td>\n",
       "      <td>0.735708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.566234</td>\n",
       "      <td>0.226771</td>\n",
       "      <td>0.323684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.419504</td>\n",
       "      <td>0.492005</td>\n",
       "      <td>0.252806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.635830</td>\n",
       "      <td>0.802380</td>\n",
       "      <td>0.941991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.215446</td>\n",
       "      <td>0.127587</td>\n",
       "      <td>0.106397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true     pred1     pred2     pred3\n",
       "0     1  0.668853  0.953560  0.735708\n",
       "1     0  0.566234  0.226771  0.323684\n",
       "2     0  0.419504  0.492005  0.252806\n",
       "3     1  0.635830  0.802380  0.941991\n",
       "4     0  0.215446  0.127587  0.106397"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# アンサンブルにだけ焦点を当てるため, モデルの学習と予測値算出処理は省略\n",
    "# サンプルデータの作成\n",
    "np.random.seed(123)\n",
    "df = pd.DataFrame({\n",
    "    \"true\": [0]*700 + [1]*300,\n",
    "    \"pred1\": np.arange(1000) + np.random.rand(1000)*1200,\n",
    "    \"pred2\": np.arange(1000) + np.random.rand(1000)*1000,\n",
    "    \"pred3\": np.arange(1000) + np.random.rand(1000)*800,\n",
    "})\n",
    "df[\"pred1\"] = np.clip(df[\"pred1\"]/df[\"pred1\"].max(), 0, 1)\n",
    "df[\"pred2\"] = np.clip(df[\"pred2\"]/df[\"pred2\"].max(), 0, 1)\n",
    "df[\"pred3\"] = np.clip(df[\"pred3\"]/df[\"pred3\"].max(), 0, 1)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df[\"true\"], random_state=123)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>pred_ensamble1</th>\n",
       "      <th>pred_ensemble1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.668853</td>\n",
       "      <td>0.953560</td>\n",
       "      <td>0.735708</td>\n",
       "      <td>0.786040</td>\n",
       "      <td>0.786040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.566234</td>\n",
       "      <td>0.226771</td>\n",
       "      <td>0.323684</td>\n",
       "      <td>0.372230</td>\n",
       "      <td>0.372230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.419504</td>\n",
       "      <td>0.492005</td>\n",
       "      <td>0.252806</td>\n",
       "      <td>0.388105</td>\n",
       "      <td>0.388105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.635830</td>\n",
       "      <td>0.802380</td>\n",
       "      <td>0.941991</td>\n",
       "      <td>0.793400</td>\n",
       "      <td>0.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.215446</td>\n",
       "      <td>0.127587</td>\n",
       "      <td>0.106397</td>\n",
       "      <td>0.149810</td>\n",
       "      <td>0.149810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true     pred1     pred2     pred3  pred_ensamble1  pred_ensemble1\n",
       "0     1  0.668853  0.953560  0.735708        0.786040        0.786040\n",
       "1     0  0.566234  0.226771  0.323684        0.372230        0.372230\n",
       "2     0  0.419504  0.492005  0.252806        0.388105        0.388105\n",
       "3     1  0.635830  0.802380  0.941991        0.793400        0.793400\n",
       "4     0  0.215446  0.127587  0.106397        0.149810        0.149810"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単純平均によるアンサンブル\n",
    "df_train[\"pred_ensemble1\"] = (df_train[\"pred1\"] + df_train[\"pred2\"] + df_train[\"pred3\"]) / 3\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc] model1:0.8029, model2: 0.8446, model3: 0.8926 -> ensemble: 0.9383\n"
     ]
    }
   ],
   "source": [
    "# アンサンブル用の精度評価関数と精度評価\n",
    "def evaluate_ensemnble(input_df, col_pred):\n",
    "    print(\"[auc] model1:{:.4f}, model2: {:.4f}, model3: {:.4f} -> ensemble: {:.4f}\".format(\n",
    "        roc_auc_score(input_df[\"true\"], input_df[\"pred1\"]),\n",
    "        roc_auc_score(input_df[\"true\"], input_df[\"pred2\"]),\n",
    "        roc_auc_score(input_df[\"true\"], input_df[\"pred3\"]),\n",
    "        roc_auc_score(input_df[\"true\"], input_df[col_pred]),))\n",
    "evaluate_ensemnble(df_train, col_pred=\"pred_ensemble1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc] model1:0.8538, model2: 0.8467, model3: 0.9212 -> ensemble: 0.9601\n"
     ]
    }
   ],
   "source": [
    "# 推論時のアンサンブル処理と精度評価\n",
    "df_test[\"pred_ensemble1\"] = (df_test[\"pred1\"] + df_test[\"pred2\"] + df_test[\"pred3\"]) / 3\n",
    "evaluate_ensemnble(df_test, col_pred=\"pred_ensemble1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.2　重み付き平均\n",
    "- 各モデルの評価値（精度）をもとに決める\n",
    "- 検証データの評価値をもとに決める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.3 0.4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>pred_ensemble2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.668853</td>\n",
       "      <td>0.953560</td>\n",
       "      <td>0.735708</td>\n",
       "      <td>0.781007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.566234</td>\n",
       "      <td>0.226771</td>\n",
       "      <td>0.323684</td>\n",
       "      <td>0.367375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.419504</td>\n",
       "      <td>0.492005</td>\n",
       "      <td>0.252806</td>\n",
       "      <td>0.374575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.635830</td>\n",
       "      <td>0.802380</td>\n",
       "      <td>0.941991</td>\n",
       "      <td>0.808259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.215446</td>\n",
       "      <td>0.127587</td>\n",
       "      <td>0.106397</td>\n",
       "      <td>0.145469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true     pred1     pred2     pred3  pred_ensemble2\n",
       "0     1  0.668853  0.953560  0.735708        0.781007\n",
       "1     0  0.566234  0.226771  0.323684        0.367375\n",
       "2     0  0.419504  0.492005  0.252806        0.374575\n",
       "3     1  0.635830  0.802380  0.941991        0.808259\n",
       "4     0  0.215446  0.127587  0.106397        0.145469"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = [0.3, 0.3, 0.4]\n",
    "weight = weight / np.sum(weight)\n",
    "print(weight)\n",
    "\n",
    "df_train[\"pred_ensemble2\"] = df_train[\"pred1\"] * weight[0] + df_train[\"pred2\"] * weight[1] + df_train[\"pred3\"] * weight[2] \n",
    "df_train[[\"true\", \"pred1\", \"pred2\", \"pred3\", \"pred_ensemble2\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc] model1:0.8029, model2: 0.8446, model3: 0.8926 -> ensemble: 0.9407\n"
     ]
    }
   ],
   "source": [
    "# アンサンブルの精度評価\n",
    "evaluate_ensemnble(df_train, col_pred=\"pred_ensemble2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc] model1:0.8538, model2: 0.8467, model3: 0.9212 -> ensemble: 0.9635\n"
     ]
    }
   ],
   "source": [
    "# 推論時のアンサンブル処理と精度評価\n",
    "df_test[\"pred_ensemble2\"] = df_test[\"pred1\"] * weight[0] + df_test[\"pred2\"] * weight[1] + df_test[\"pred3\"] * weight[2] \n",
    "evaluate_ensemnble(df_test, col_pred=\"pred_ensemble2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.3　スタッキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>pred_ensemble3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.668853</td>\n",
       "      <td>0.953560</td>\n",
       "      <td>0.735708</td>\n",
       "      <td>0.685202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.566234</td>\n",
       "      <td>0.226771</td>\n",
       "      <td>0.323684</td>\n",
       "      <td>0.053733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.419504</td>\n",
       "      <td>0.492005</td>\n",
       "      <td>0.252806</td>\n",
       "      <td>0.074234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.635830</td>\n",
       "      <td>0.802380</td>\n",
       "      <td>0.941991</td>\n",
       "      <td>0.769464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.215446</td>\n",
       "      <td>0.127587</td>\n",
       "      <td>0.106397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true     pred1     pred2     pred3  pred_ensemble3\n",
       "0     1  0.668853  0.953560  0.735708        0.685202\n",
       "1     0  0.566234  0.226771  0.323684        0.053733\n",
       "2     0  0.419504  0.492005  0.252806        0.074234\n",
       "3     1  0.635830  0.802380  0.941991        0.769464\n",
       "4     0  0.215446  0.127587  0.106397        0.000000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スタッキングによるアンサンブル\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "x, y = df_train[[\"pred1\", \"pred2\", \"pred3\"]], df_train[[\"true\"]]\n",
    "oof = np.zeros(len(x))\n",
    "models = []\n",
    "\n",
    "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(x, y))\n",
    "for nfold in np.arange(5):\n",
    "    # 学習データと検証データの分離\n",
    "    idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "    x_tr, y_tr = x.loc[idx_tr, :], y.loc[idx_tr, :]\n",
    "    x_va, y_va = x.loc[idx_va, :], y.loc[idx_va, :]\n",
    "    # モデル学習\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(x_tr, y_tr)\n",
    "    models.append(model)\n",
    "    # 検証データの予測値算出\n",
    "    y_va_pred = model.predict(x_va)\n",
    "    oof[idx_va] = y_va_pred\n",
    "    \n",
    "df_train[\"pred_ensemble3\"] = oof\n",
    "df_train[\"pred_ensemble3\"] = df_train[\"pred_ensemble3\"].clip(lower=0, upper=1)\n",
    "df_train[[\"true\", \"pred1\", \"pred2\", \"pred3\", \"pred_ensemble3\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc] model1:0.8029, model2: 0.8446, model3: 0.8926 -> ensemble: 0.9398\n"
     ]
    }
   ],
   "source": [
    "evaluate_ensemnble(df_train, col_pred=\"pred_ensemble3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc] model1:0.8538, model2: 0.8467, model3: 0.9212 -> ensemble: 0.9662\n"
     ]
    }
   ],
   "source": [
    "df_test[\"pred_ensemble3\"] = 0\n",
    "for model in models:\n",
    "    df_test[\"pred_ensemble3\"] += model.predict(df_test[[\"pred1\", \"pred2\", \"pred3\"]]) / len(models)\n",
    "df_test[\"pred_ensemble3\"] = df_test[\"pred_ensemble3\"].clip(lower=0, upper=1)\n",
    "evaluate_ensemnble(df_test, col_pred=\"pred_ensemble3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "153e4705b1c652da88ea7ffc880aa7ae6c4b1c14ad6096906ddc957e589b7347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
